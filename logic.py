import json
import os
import urllib.parse
from pypdf import PdfReader
import unicodedata
import re
from io import BytesIO
from docx import Document
from docx.shared import Pt, RGBColor, Cm
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_TAB_ALIGNMENT, WD_TAB_LEADER
from docx.oxml.ns import qn
from docx.oxml import OxmlElement
from weasyprint import HTML, CSS

# Sistema de logging unificado
from logging_config import setup_logger
logger = setup_logger("VANT_LOGIC")

# ==============================================================================
# HELPER FUNCTIONS (ESSENCIAIS PARA O PARSER)
# ==============================================================================

def safe_txt(text):
    """
    Limpa caracteres quebrados de encoding e normaliza espa√ßos.
    
    Args:
        text (str): Texto bruto a ser limpo.
    
    Returns:
        str: Texto limpo com caracteres especiais substitu√≠dos e espa√ßos normalizados.
    """
    if not text: return ""
    replacements = {
        '‚Äì': '-', '‚Äî': '-', '‚Äú': '"', '‚Äù': '"', '‚Äò': "'", '‚Äô': "'", 
        '‚Ä¢': '', '‚óè': '', '‚ö´': '', '‚ñ†': '', '\u200b': '', '\t': ' ', '*': '' 
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    
    # Remove espa√ßos duplos
    text = re.sub(r'\s{2,}', ' ', text)
    # Remove markdown residual se necess√°rio
    text = text.replace('**', '').replace('__', '').replace('#', '')
    if text.strip().startswith('- '): text = text.strip()[2:]
    
    return text.strip()

def clean_garbage(text):
    """Remove termos como (n√£o informado) e sujeira de OCR."""
    if not text: return ""
    garbage = [
        r'\(?n[√£a]o\s*informado\)?', r'\(?n[√£a]o\s*fornecido\)?', 
        r'\(?sem\s*telefone\)?', r'\(?undefined\)?', r'null'
    ]
    clean = text
    for p in garbage: 
        clean = re.sub(p, '', clean, flags=re.IGNORECASE)
    
    clean = clean.strip().rstrip('|').strip()
    
    # Se sobrou apenas o r√≥tulo, retorna vazio
    if clean.lower() in ['telefone:', 'email:', 'linkedin:', 'local:', 'e', 'a']: 
        return ""
        
    return clean

def normalize_visual(text):
    if not text:
        return ""

    # Remove Markdown estrutural (t√≠tulos, bold, etc)
    text = re.sub(r'^#+\s*', '', text)   # remove #, ##, ### no in√≠cio
    text = re.sub(r'[*_`]+', '', text)   # remove ** __ `

    # Normaliza√ß√µes visuais que voc√™ j√° tem
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r'\s{2,}', ' ', text)

    return text.strip()



def extract_date(text):
    """Extrai padr√µes de data (ex: Jan 2020 - Atual) e retorna o resto do texto."""
    # Padr√µes comuns de data em CVs
    patterns = [
        r'(\b(?:jan|fev|mar|abr|mai|jun|jul|ago|set|out|nov|dez)[a-z]*\s*(?:de)?\s*\d{4}\s*[-‚Äì]\s*(?:presente|atual|\w+\s*\d{4}))',
        r'(\d{2}/\d{4}\s*[-‚Äì]\s*(?:presente|atual|\d{2}/\d{4}))',
        r'(\d{4}\s*[-‚Äì]\s*(?:presente|atual|\d{4}))'
    ]
    found = ""
    text_clean = text
    
    for pat in patterns:
        m = re.search(pat, text, re.IGNORECASE)
        if m:
            found = m.group(1).strip()
            # Remove a data do texto original para sobrar s√≥ o Cargo
            text_clean = text.replace(m.group(0), '').strip()
            # Limpa separadores que sobraram no fim
            if text_clean.endswith('|') or text_clean.endswith('-'): 
                text_clean = text_clean[:-1].strip()
            break
            
    return text_clean, found

# ============================================================
# IMPORTA A INTELIG√äNCIA
# ============================================================
from llm_core import run_llm_orchestrator

# ============================================================
# CONFIG
# ============================================================
# Configura√ß√£o de logging j√° feita via setup_logger

# ============================================================
# PDF / TEXTO (COM LIMITE DE TOKENS)
# ============================================================
def extrair_texto_pdf(uploaded_file, max_chars=25000):
    """
    Extrai texto e verifica se o arquivo foi gerado pelo VANT 
    (via Metadado ou Marca d'√°gua no texto) para ativar o modo 'Certified'.
    """
    try:
        reader = PdfReader(uploaded_file)
        
        full_text = ""
        for page in reader.pages:
            full_text += page.extract_text() or ""

        # [TECH LEAD MAGIC] - Detec√ß√£o da Assinatura Digital
        is_vant_certified = False
        
        # Check A: Metadado (Para Upload Direto do PDF gerado pelo sistema)
        if reader.metadata and reader.metadata.get('/Creator') == "VANT_NEURAL_ENGINE_V2":
            is_vant_certified = True
            logger.info("üíé VANT CERTIFIED: Detectado via Metadado PDF!")
            
        # Check B: Marca d'√°gua no Texto (Caso o usu√°rio tenha Copiado/Colado no Word)
        elif "VANT-ID: VANT-NEURAL-ENGINE-CERTIFIED" in full_text:
            is_vant_certified = True
            logger.info("üíé VANT CERTIFIED: Detectado via Texto/Watermark!")

        # Inje√ß√£o de Contexto para a IA
        # Se for certificado, injetamos o cabe√ßalho que garante nota alta de estrutura/ATS.
        if is_vant_certified:
            header_magic = "\n[SYSTEM_INTERNAL_NOTE: ESTE CURR√çCULO J√Å FOI OTIMIZADO PELO VANT. A ESTRUTURA E FORMATACAO ESTAO PERFEITAS (NOTA ATS DEVE SER > 90 NA PARTE ESTRUTURAL). AVALIE APENAS A ADERENCIA DAS SKILLS.]\n"
            full_text = header_magic + full_text

        if len(full_text) > max_chars:
            logger.info(f"‚ö†Ô∏è PDF truncado em {max_chars} caracteres para seguran√ßa.")
            return full_text[:max_chars]
            
        return full_text
        
    except Exception as e:
        logger.error(f"Erro cr√≠tico ao ler PDF: {e}")
        return ""

# ============================================================
# CAT√ÅLOGO DE ELITE (V5.0 - LIBRARY EXPANDIDA)
# Fonte da Verdade para Fallback & Curadoria IA
# ============================================================
BACKUP_CATALOG = {
    "ti_dev_gen": [
        # Engenharia de Software & C√≥digo
        {"titulo": "Clean Code", "autor": "Robert C. Martin", "motivo": "Diferencia o amador do profissional. Essencial para passar nos Code Reviews de grandes empresas."},
        {"titulo": "O Programador Pragm√°tico", "autor": "Andrew Hunt", "motivo": "Vai te ensinar a pensar como um Engenheiro de Software S√™nior, n√£o apenas como um codificador."},
        {"titulo": "Arquitetura Limpa", "autor": "Robert C. Martin", "motivo": "Leitura obrigat√≥ria se voc√™ quer desenhar sistemas escal√°veis e n√£o apenas consertar bugs."},
        {"titulo": "Refatora√ß√£o", "autor": "Martin Fowler", "motivo": "A t√©cnica vital para trabalhar com c√≥digo legado sem quebrar o sistema inteiro."},
        {"titulo": "Padr√µes de Projeto (Design Patterns)", "autor": "Erich Gamma", "motivo": "A linguagem universal dos arquitetos de software. Domine isso para discutir solu√ß√µes t√©cnicas."},
        
        # Carreira & Algoritmos
        {"titulo": "Entendendo Algoritmos", "autor": "Aditya Bhargava", "motivo": "O guia ilustrado definitivo para perder o medo de l√≥gica complexa e otimiza√ß√£o."},
        {"titulo": "System Design Interview", "autor": "Alex Xu", "motivo": "A chave mestra para ser aprovado em entrevistas t√©cnicas de Big Techs e cargos de lideran√ßa."},
        {"titulo": "Cracking the Coding Interview", "autor": "Gayle Laakmann", "motivo": "O manual de sobreviv√™ncia para passar nos testes t√©cnicos mais dif√≠ceis do mercado."},
        
        # DevOps & Cultura
        {"titulo": "O Projeto F√™nix", "autor": "Gene Kim", "motivo": "Entenda como a TI impacta o neg√≥cio. Leitura obrigat√≥ria para quem busca cargos de DevOps ou Gest√£o."},
        {"titulo": "Engenharia de Confiabilidade (SRE)", "autor": "Google", "motivo": "Como o Google mant√©m seus sistemas no ar. A b√≠blia para infraestrutura e sustenta√ß√£o."}
    ],
    
    # NOVA CATEGORIA: SUPORTE E INFRAESTRUTURA
    "ti_suporte": [
        {"titulo": "Google IT Support Professional", "autor": "Google", "motivo": "A base fundamental para suporte moderno e troubleshooting eficaz."},
        {"titulo": "ITIL 4 Foundation", "autor": "Axelos", "motivo": "O padr√£o mundial para gerenciamento de servi√ßos de TI. Essencial para empresas grandes."},
        {"titulo": "Redes de Computadores", "autor": "Tanenbaum", "motivo": "A b√≠blia t√©cnica para entender TCP/IP e resolver problemas de conectividade."},
        {"titulo": "Windows Internals", "autor": "Pavel Yosifovich", "motivo": "Para quem precisa resolver problemas profundos no SO que o reboot n√£o resolve."},
        {"titulo": "The Service Desk Handbook", "autor": "Sanjay Nagaraj", "motivo": "Focado em atendimento ao cliente t√©cnico e SLAs."},
        {"titulo": "Comece pelo Porqu√™", "autor": "Simon Sinek", "motivo": "Soft skill vital: entender a dor do usu√°rio antes de mexer no computador."}
    ],

    "ti_dados_ai": [
        # Estrat√©gia & Comunica√ß√£o
        {"titulo": "Storytelling com Dados", "autor": "Cole Nussbaumer", "motivo": "Transforma voc√™ de um analista de planilhas em um parceiro estrat√©gico que influencia diretores."},
        {"titulo": "Data Science para Neg√≥cios", "autor": "Foster Provost", "motivo": "Para entender como os dados geram dinheiro de verdade nas empresas, al√©m do algoritmo."},
        {"titulo": "Como Mentir com Estat√≠stica", "autor": "Darrell Huff", "motivo": "Blinde suas an√°lises contra vieses e ganhe confian√ßa absoluta nos seus relat√≥rios."},
        
        # T√©cnico & M√£o na Massa
        {"titulo": "M√£os √† Obra: Aprendizado de M√°quina", "autor": "Aur√©lien G√©ron", "motivo": "O manual pr√°tico definitivo para sair da teoria e colocar modelos de IA em produ√ß√£o real."},
        {"titulo": "Python para An√°lise de Dados", "autor": "Wes McKinney", "motivo": "Escrito pelo criador do Pandas. √â a refer√™ncia t√©cnica para manipula√ß√£o de dados pesada."},
        {"titulo": "Deep Learning Book", "autor": "Ian Goodfellow", "motivo": "A refer√™ncia acad√™mica m√°xima se voc√™ quer mergulhar fundo em Redes Neurais."},
        {"titulo": "Designing Data-Intensive Applications", "autor": "Martin Kleppmann", "motivo": "O livro mais respeitado do mundo sobre como construir sistemas de dados robustos e distribu√≠dos."}
    ],

    "produto_agil": [
        # Gest√£o de Produto
        {"titulo": "Inspirado (Inspired)", "autor": "Marty Cagan", "motivo": "O livro de cabeceira dos melhores Product Managers do Vale do Sil√≠cio. Define o padr√£o da ind√∫stria."},
        {"titulo": "Empowered", "autor": "Marty Cagan", "motivo": "Leitura cr√≠tica se voc√™ busca sair da execu√ß√£o de tarefas para atuar com autonomia e lideran√ßa de produto."},
        {"titulo": "Hooked (Engajado)", "autor": "Nir Eyal", "motivo": "A psicologia por tr√°s dos produtos que ret√™m usu√°rios. Essencial para m√©tricas de reten√ß√£o."},
        {"titulo": "Continuous Discovery Habits", "autor": "Teresa Torres", "motivo": "Como descobrir o que o cliente realmente quer, de forma cont√≠nua, sem 'achismos'."},
        
        # Metodologia & Estrat√©gia
        {"titulo": "A Startup Enxuta (Lean Startup)", "autor": "Eric Ries", "motivo": "A base fundamental de valida√ß√£o. Aprenda a errar r√°pido para acertar o produto."},
        {"titulo": "Sprint", "autor": "Jake Knapp", "motivo": "A metodologia do Google para validar ideias em 5 dias, evitando meses de desenvolvimento in√∫til."},
        {"titulo": "Scrum: A Arte de Fazer o Dobro...", "autor": "Jeff Sutherland", "motivo": "A origem da metodologia √°gil. Essencial para organizar times ca√≥ticos."},
        {"titulo": "Crossing the Chasm", "autor": "Geoffrey Moore", "motivo": "A estrat√©gia definitiva para lan√ßar produtos de tecnologia em mercados B2B."}
    ],

    "marketing_growth": [
        # Estrat√©gia Digital & Growth
        {"titulo": "Hacking Growth", "autor": "Sean Ellis", "motivo": "A metodologia exata usada para escalar startups unic√≥rnio. Saia do 'achismo' para os testes r√°pidos."},
        {"titulo": "Marketing 4.0", "autor": "Philip Kotler", "motivo": "Atualize seu mindset para as estrat√©gias digitais modernas com a maior autoridade do mundo."},
        {"titulo": "Traction", "autor": "Gabriel Weinberg", "motivo": "Um guia pr√°tico com 19 canais de aquisi√ß√£o para voc√™ nunca ficar sem clientes."},
        {"titulo": "Dotcom Secrets", "autor": "Russell Brunson", "motivo": "A engenharia por tr√°s de funis de vendas online que convertem visitantes em compradores."},
        
        # Psicologia & Branding
        {"titulo": "As Armas da Persuas√£o", "autor": "Robert Cialdini", "motivo": "Domine os gatilhos mentais para aumentar sua convers√£o em qualquer campanha ou negocia√ß√£o."},
        {"titulo": "Cont√°gio: Por que as Coisas Pegam", "autor": "Jonah Berger", "motivo": "A ci√™ncia da viraliza√ß√£o. Entenda por que alguns conte√∫dos explodem e outros morrem."},
        {"titulo": "StoryBrand", "autor": "Donald Miller", "motivo": "Aprenda a comunicar o valor da sua marca de forma t√£o clara que o cliente n√£o consegue ignorar."},
        {"titulo": "Posicionamento", "autor": "Al Ries", "motivo": "O cl√°ssico sobre como ocupar um lugar √∫nico na mente do seu consumidor."}
    ],

    "vendas_cs": [
        # B2B & Vendas Complexas
        {"titulo": "Receita Previs√≠vel", "autor": "Aaron Ross", "motivo": "O blueprint para construir uma m√°quina de vendas B2B escal√°vel e sair da depend√™ncia de indica√ß√£o."},
        {"titulo": "Spin Selling", "autor": "Neil Rackham", "motivo": "A t√©cnica definitiva para fechar contratos de alto valor (Enterprise) fazendo as perguntas certas."},
        {"titulo": "A Venda Desafiadora", "autor": "Matthew Dixon", "motivo": "Para vender solu√ß√µes complexas, voc√™ precisa desafiar o cliente e ensinar algo novo, n√£o apenas agradar."},
        {"titulo": "Negocie Como se Sua Vida Dependesse Disso", "autor": "Chris Voss", "motivo": "T√©cnicas de negocia√ß√£o do FBI aplicadas ao mundo corporativo de alto risco."},
        
        # Atitude & Reten√ß√£o
        {"titulo": "Customer Success", "autor": "Dan Steinman", "motivo": "Leitura obrigat√≥ria para entender que a venda real come√ßa ap√≥s a assinatura do contrato."},
        {"titulo": "A B√≠blia de Vendas", "autor": "Jeffrey Gitomer", "motivo": "O guia pr√°tico de atitude e t√©cnica para quem vive de comiss√£o."},
        {"titulo": "Fanatical Prospecting", "autor": "Jeb Blount", "motivo": "Acabe com o problema de pipeline vazio. O guia bruto sobre prospec√ß√£o ativa."}
    ],

    "rh_lideranca": [
        # Gest√£o & Cultura
        {"titulo": "Work Rules!", "autor": "Laszlo Bock", "motivo": "Os segredos de gest√£o de pessoas que fizeram do Google uma refer√™ncia mundial em talentos."},
        {"titulo": "High Output Management", "autor": "Andrew Grove", "motivo": "A b√≠blia da gest√£o de alta performance escrita pelo lend√°rio CEO da Intel."},
        {"titulo": "Os 5 Desafios das Equipes", "autor": "Patrick Lencioni", "motivo": "Entenda a din√¢mica oculta que impede seu time de atingir a alta performance."},
        
        # Desenvolvimento de L√≠deres
        {"titulo": "Pipeline de Lideran√ßa", "autor": "Ram Charan", "motivo": "O mapa claro do que √© exigido em cada degrau da escada corporativa para voc√™ ser promovido."},
        {"titulo": "Radical Candor", "autor": "Kim Scott", "motivo": "Aprenda a dar feedbacks dif√≠ceis sem destruir a rela√ß√£o com seu time. Vital para gestores."},
        {"titulo": "A Coragem de Ser Imperfeito", "autor": "Bren√© Brown", "motivo": "Como a vulnerabilidade pode ser sua maior for√ßa na lideran√ßa de equipes modernas."},
        {"titulo": "Comece pelo Porqu√™", "autor": "Simon Sinek", "motivo": "Fundamental para l√≠deres que precisam inspirar a√ß√£o e prop√≥sito, n√£o apenas dar ordens."}
    ],

    "financeiro_corp": [
        # Finan√ßas T√©cnicas
        {"titulo": "Valuation", "autor": "Aswath Damodaran", "motivo": "A b√≠blia t√©cnica para quem quer falar a l√≠ngua dos CFOs e investidores com propriedade."},
        {"titulo": "O Investidor Inteligente", "autor": "Benjamin Graham", "motivo": "Fundamentos de an√°lise financeira que sobrevivem a qualquer crise de mercado."},
        {"titulo": "Financial Intelligence", "autor": "Karen Berman", "motivo": "Contabilidade e finan√ßas explicadas para gestores que n√£o s√£o da √°rea financeira."},
        
        # Mindset & Neg√≥cios
        {"titulo": "Princ√≠pios", "autor": "Ray Dalio", "motivo": "Como sistematizar a cultura e a tomada de decis√£o para atingir resultados excepcionais."},
        {"titulo": "A Psicologia do Dinheiro", "autor": "Morgan Housel", "motivo": "Entenda como o comportamento humano afeta as decis√µes financeiras mais do que a matem√°tica."},
        {"titulo": "Pai Rico, Pai Pobre", "autor": "Robert Kiyosaki", "motivo": "Essencial para desenvolver a mentalidade de ativos vs passivos na gest√£o de patrim√¥nio."},
        {"titulo": "A Marca da Vit√≥ria (Shoe Dog)", "autor": "Phil Knight", "motivo": "A jornada real e ca√≥tica de construir uma empresa global (Nike) do zero."}
    ],

    "construcao_manual": [
        {"titulo": "Manual do Construtor", "autor": "V√°rios", "motivo": "A refer√™ncia t√©cnica para executar obras com padr√£o de engenharia, evitando retrabalho e desperd√≠cio."},
        {"titulo": "Normas Regulamentadoras (NRs Comentadas)", "autor": "Editora Saraiva", "motivo": "Domine as normas de seguran√ßa para ser o profissional mais confi√°vel e requisitado da obra."},
        {"titulo": "Instala√ß√µes El√©tricas Prediais", "autor": "H√©lio Creder", "motivo": "O guia definitivo para eletricistas que querem garantir seguran√ßa e conformidade t√©cnica."},
        {"titulo": "Mestre de Obras: Gest√£o B√°sica", "autor": "Senai", "motivo": "O passo fundamental para deixar de ser operacional e come√ßar a liderar equipes no canteiro."},
        {"titulo": "Concreto Armado Eu Te Amo", "autor": "Manoel Botelho", "motivo": "A melhor did√°tica do mercado para entender estruturas sem complica√ß√£o matem√°tica."}
    ],

    "gastronomia": [
        {"titulo": "Kitchen Confidential", "autor": "Anthony Bourdain", "motivo": "Entenda a realidade bruta, a hierarquia e a disciplina necess√°ria numa cozinha de alta performance."},
        {"titulo": "The Professional Chef", "autor": "CIA", "motivo": "A b√≠blia t√©cnica culin√°ria. Diferencia o cozinheiro amador do Chef profissional."},
        {"titulo": "Sal, Gordura, √Åcido, Calor", "autor": "Samin Nosrat", "motivo": "Domine os 4 elementos fundamentais para criar sabor sem depender apenas de receitas prontas."},
        {"titulo": "Larousse Gastronomique", "autor": "Prosper Montagn√©", "motivo": "A enciclop√©dia definitiva da gastronomia mundial. Refer√™ncia absoluta."}
    ],

    "global_soft_skills": [
        {"titulo": "H√°bitos At√¥micos", "autor": "James Clear", "motivo": "Pequenas mudan√ßas de rotina que comp√µem resultados gigantes na sua carreira a longo prazo."},
        {"titulo": "Trabalho Focado (Deep Work)", "autor": "Cal Newport", "motivo": "A habilidade mais rara do s√©culo XXI: focar sem distra√ß√£o para produzir trabalho de elite."},
        {"titulo": "Essencialismo", "autor": "Greg McKeown", "motivo": "Aprenda a dizer 'n√£o' para o que √© apenas bom e focar energia exclusivamente no que √© excelente."},
        {"titulo": "Como Fazer Amigos e Influenciar Pessoas", "autor": "Dale Carnegie", "motivo": "A habilidade n√∫mero 1 para crescer na pol√≠tica corporativa, vendas e networking."},
        {"titulo": "Comunica√ß√£o N√£o-Violenta", "autor": "Marshall Rosenberg", "motivo": "A ferramenta essencial para resolver conflitos e negociar em ambientes de alta press√£o."},
        {"titulo": "Mindset", "autor": "Carol Dweck", "motivo": "A chave psicol√≥gica para aceitar desafios e crescer profissionalmente."},
        {"titulo": "O Ego √© seu Inimigo", "autor": "Ryan Holiday", "motivo": "Como impedir que sua arrog√¢ncia destrua sua carreira nos momentos de sucesso."}
    ]
}

_BOOKS_CATALOG_CACHE = None

def load_books_catalog():
    global _BOOKS_CATALOG_CACHE

    if _BOOKS_CATALOG_CACHE is not None:
        return _BOOKS_CATALOG_CACHE

    base_dir = os.path.dirname(os.path.abspath(__file__))
    catalog_path = os.path.join(base_dir, "data", "books_catalog.json")

    try:
        if os.path.exists(catalog_path):
            with open(catalog_path, encoding="utf-8") as f:
                _BOOKS_CATALOG_CACHE = json.load(f)
                return _BOOKS_CATALOG_CACHE
        else:
            return {}
    except Exception as e:
        logger.error(f"Erro ao carregar cat√°logo JSON: {e}")
        return {}

# ============================================================
# DETECTOR DE √ÅREA (REGEX OTIMIZADO)
# ============================================================

def detect_job_area(job_description):
    job_lower = job_description.lower()
    
    keyword_map = {
        "ti_dados_ai": [
            r"dados", r"data", r"analytics", r"bi\b", r"business intelligence", r"cientista de dados", 
            r"machine learning", r"ia\b", r"intelig√™ncia artificial", r"python", r"pandas", r"sql", r"big data"
        ],
        "ti_suporte": [
            r"suporte", r"help desk", r"service desk", r"infraestrutura", r"infra", r"sysadmin", 
            r"t√©cnico de ti", r"n1", r"n2", r"field service", r"atendimento", r"hardware", r"redes"
        ],
        "ti_dev_gen": [
            r"desenvolvedor", r"developer", r"engenheiro de software", r"software engineer", 
            r"fullstack", r"backend", r"frontend", r"java\b", r"python", r"react"
        ],
        "produto_agil": [
            r"produto", r"product manager", r"product owner", r"po\b", r"pm\b", r"scrum", r"agile", r"kanban", 
            r"agilista", r"roadmap", r"backlog", r"user story"
        ],
        "marketing_growth": [
            r"marketing", r"growth", r"performance", r"tr√°fego", r"seo\b", r"conte√∫do", r"social media", 
            r"branding", r"copywriter", r"crm\b", r"inbound", r"redator", r"designer"
        ],
        "vendas_cs": [
            r"vendas", r"sales", r"comercial", r"sdr\b", r"bdr\b", r"closer", r"executivo de contas", r"account executive", 
            r"customer success", r"sucesso do cliente", r"p√≥s-venda", r"churn", r"negocia√ß√£o"
        ],
        "rh_lideranca": [
            r"rh\b", r"r\.h\.", r"recursos humanos", r"recrutamento", r"talent", r"people", r"dp\b", r"departamento pessoal", 
            r"tech recruiter", r"bp\b", r"business partner", r"lideran√ßa", r"gest√£o de pessoas", r"coordenador", r"gerente", r"supervisor", 
            r"analista de rh" 
        ],
        "financeiro_corp": [
            r"financeiro", r"finan√ßas", r"cont√°bil", r"contabilidade", r"fiscal", r"auditoria", r"controller", 
            r"fp&a", r"tesouraria", r"banco", r"investimento", r"fus√µes", r"economista"
        ],
        "construcao_manual": [
            r"pedreiro", r"servente", r"mestre de obras", r"obra", r"civil", r"constru√ß√£o", 
            r"el√©trica", r"eletricista", r"manuten√ß√£o", r"engenheiro civil", r"arquitetura"
        ],
        "gastronomia": [
            r"cozinha", r"chef", r"gastronomia", r"culin√°ria", r"restaurante", r"alimento", r"cook"
        ]
    }
    
    best_match = "global_soft_skills"
    max_count = 0
    
    for area, patterns in keyword_map.items():
        count = 0
        for pattern in patterns:
            if re.search(pattern, job_lower):
                count += 1
        
        if count > max_count:
            max_count = count
            best_match = area
            
    return best_match

def format_text_to_html_diagnostic(text):
    """
    Vers√£o espec√≠fica para blocos de diagn√≥stico (mant√©m cores vibrantes).
    Diferente do Paper View que usa preto.
    """
    if not text: return ""
    
    # Escapa HTML
    text = text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    
    # Negrito ‚Üí Ciano Neon (cor de destaque do sistema)
    text = re.sub(r'\*\*(.*?)\*\*', r'<strong style="color: #38BDF8; font-weight: 700;">\1</strong>', text)
    
    # It√°lico
    text = re.sub(r'(?<!\*)\*(?!\*)(.*?)\*', r'<em style="color: #94A3B8; font-style: normal; font-weight: 600;">\1</em>', text)
    
    # Quebras de linha
    text = text.replace('\n', '<br>')
    
    return text

# ============================================================
# FORMATADOR HTML V11 (STABLE LAYOUT - DUAL ROW)
# ============================================================
def format_text_to_html(text):
    if not text: return ""
    
    text = text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    html_output = []
    lines = text.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line: continue
            
        # 1. NOME & 2. SE√á√ïES (Manter igual)
        if line.startswith('# '):
            clean = line.replace('# ', '').upper()
            html_output.append(f'<h1 class="vant-cv-name">{clean}</h1>')
        elif line.startswith('###'):
            clean = line.replace('###', '').strip().upper()
            html_output.append(f'<h2 class="vant-cv-section">{clean}</h2>')
            
        # 3. TRATAMENTO DE LISTAS (CARGOS vs TAREFAS)
        elif line.startswith('- ') or line.startswith('* ') or line.startswith('‚Ä¢ '):
            clean = re.sub(r'^[-*‚Ä¢]\s+', '', line)
            clean = re.sub(r'\*\*(.*?)\*\*', r'<span class="vant-bold">\1</span>', clean)
            
            # --- L√ìGICA DE HEADER DE VAGA (ESTRUTURA DE 2 LINHAS) ---
            if '|' in line:
                parts = [p.strip() for p in clean.split('|')]
                
                # Vari√°veis padr√£o
                cargo = clean
                empresa = ""
                data = ""
                
                if len(parts) >= 3:
                    cargo = parts[0]
                    empresa = parts[1]
                    data = parts[2].replace('*', '').replace('_', '').strip()
                elif len(parts) == 2:
                    cargo = parts[0]
                    empresa = parts[1].replace('*', '').strip()
                
                # Montagem do HTML com quebra for√ßada
                job_html = f"""
                <div class="vant-cv-job-container">
                    <div class="vant-job-row-primary">
                        <span class="vant-job-title">{cargo}</span>
                        <span class="vant-job-sep">|</span>
                        <span class="vant-job-company">{empresa}</span>
                    </div>
                    """
                
                # S√≥ adiciona a linha da data se ela existir
                if data:
                    job_html += f"""
                    <div class="vant-job-row-secondary">
                        <span class="vant-job-date">{data}</span>
                    </div>
                    """
                
                job_html += "</div>"
                html_output.append(job_html)
            
            # --- L√ìGICA DE TAREFAS (Manter igual) ---
            else:
                row = f"""
                <div class="vant-cv-grid-row">
                    <div class="vant-cv-bullet-col">‚Ä¢</div>
                    <div class="vant-cv-text-col">{clean}</div>
                </div>
                """
                html_output.append(row)

        # 4. CONTATOS & 5. TEXTO CORRIDO (Manter igual)
        elif ('|' in line or '@' in line) and len(line) < 300:
            clean_line = line.replace('**', '')
            parts = [p.strip() for p in clean_line.split('|')]
            items_html = []
            for p in parts:
                if p:
                    if ":" in p:
                        label, val = p.split(":", 1)
                        block = f'<span class="vant-contact-block"><span class="vant-bold">{label}:</span> {val}</span>'
                        items_html.append(block)
                    else:
                        items_html.append(f'<span class="vant-contact-block">{p}</span>')
            full_html = '<span class="vant-contact-separator"> ‚Ä¢ </span>'.join(items_html)
            html_output.append(f'<div class="vant-cv-contact-line">{full_html}</div>')
        else:
            clean = re.sub(r'\*\*(.*?)\*\*', r'<span class="vant-bold">\1</span>', line)
            html_output.append(f'<p class="vant-cv-paragraph">{clean}</p>')
            
    return "\n".join(html_output)

# ============================================================
# LINK AMAZON INTELIGENTE
# ============================================================
def gerar_link_amazon(titulo_livro, autor=None):
    tag_afiliado = "rodrigoverruc-20"
    
    if isinstance(titulo_livro, dict) and "amazon_url" in titulo_livro:
        return titulo_livro["amazon_url"]
        
    try:
        titulo_limpo = titulo_livro.split(":")[0] 
        termo_busca = titulo_limpo
        if autor:
            autor_limpo = autor.split(",")[0] 
            termo_busca = f"{titulo_limpo} {autor_limpo}"
            
        query = urllib.parse.quote(termo_busca)
        return f"https://www.amazon.com.br/s?k={query}&tag={tag_afiliado}"
        
    except Exception:
        return f"https://www.amazon.com.br/s?k=livros+profissionais&tag={tag_afiliado}"

# ============================================================
# GERADOR DE PDF (VERS√ÉO FINAL COM WEASYPRINT)
# ============================================================
# GERADOR DE PDF (VERS√ÉO FINAL COM WEASYPRINT)
# ============================================================
from weasyprint import HTML, CSS
import re
import unicodedata

# Importa constantes CSS centralizadas
from backend.styles import CSS_V13, CSS_PDF
# ==============================================================================
# 1. PARSER ENGINE (Mantido para limpar seus dados brutos)
# ==============================================================================

def parse_raw_data_to_struct(raw_text):
    """
    C√âREBRO DO SISTEMA: Transforma texto bruto em dados estruturados.
    Vers√£o Fiel: Mant√©m r√≥tulos (Email:, Telefone:) exatamente como aparecem.
    """
    raw_lines = raw_text.split('\n')
    parsed = {"name": "", "contacts": [], "sections": []}
    curr_sec = None
    buffer = []

    def flush():
        nonlocal curr_sec
        if not curr_sec:
            buffer.clear()
            return
        
        # SKILLS
        if any(k in curr_sec for k in ["SKILLS", "COMPET√äNCIAS"]):
            txt = " ".join(buffer).replace('\n', ' ')
            if '|' in txt:
                s = [safe_txt(x) for x in txt.split('|') if safe_txt(x)]
            else:
                s = [safe_txt(x) for x in re.split(r'\s{2,}', txt) if safe_txt(x)]

            if s:
                parsed["sections"].append({
                    "title": curr_sec,
                    "type": "chips",
                    "content": s
                })

        # RESUMO
        elif "RESUMO" in curr_sec:
            full = " ".join([safe_txt(x) for x in buffer if safe_txt(x)])
            if full:
                parsed["sections"].append({
                    "title": curr_sec,
                    "type": "paragraph",
                    "content": full
                })

        # LISTAS SIMPLES
        elif any(k in curr_sec for k in ["IDIOMA", "CERTIFICA"]):
            l = [safe_txt(x) for x in buffer if safe_txt(x)]
            if l:
                parsed["sections"].append({
                    "title": curr_sec,
                    "type": "list_simple",
                    "content": l
                })

        # JOBS / EXPERI√äNCIA
        else:
            job_blocks = []
            clean_lines = [safe_txt(x) for x in buffer if safe_txt(x)]
            i = 0

            while i < len(clean_lines):
                line = clean_lines[i]

                # REGRA DE OURO:
                # S√≥ √© cabe√ßalho de cargo se tiver "|"
                is_job_header = '|' in line

                if is_job_header:
                    text_rem, date_found = extract_date(line)

                    # Divide cargo | empresa
                    parts = text_rem.split('|', 1)
                    cargo = parts[0].strip()
                    empresa = parts[1].strip() if len(parts) > 1 else ""

                    job_blocks.append({
                        "cargo": cargo,
                        "empresa": empresa,
                        "date": date_found,
                        "details": []
                    })

                else:
                    # Tudo que N√ÉO √© cabe√ßalho vira detalhe
                    if job_blocks:
                        clean_detail = line.strip()

                        # ignora lixo visual
                        if clean_detail not in ['.', '-', '‚Ä¢', '|']:
                            job_blocks[-1]["details"].append(clean_detail)

                i += 1

            # SALVA UMA √öNICA VEZ
            if job_blocks:
                parsed["sections"].append({
                    "title": curr_sec,
                    "type": "jobs",
                    "content": job_blocks
                })

        buffer.clear()


    # ================= LOOP PRINCIPAL =================
    for line in raw_lines:
        raw = line.strip()
        if not raw:
            continue

        # NOME ‚Äî apenas normaliza√ß√£o visual
        if not parsed["name"]:
            parsed["name"] = normalize_visual(raw)
            continue

        # CONTATOS ‚Äî C√ìPIA LITERAL DA LINHA INTEIRA
        if not parsed["sections"] and (
            "@" in raw or
            "linkedin" in raw.lower() or
            "telefone" in raw.lower() or
            "local" in raw.lower()
        ):
            parsed["contacts"].append(raw.strip())
            continue


        # A PARTIR DAQUI, LIMPEZA SEM√ÇNTICA NORMAL
        clean = clean_garbage(safe_txt(raw))

        # üî• EXCE√á√ÉO PARA IDIOMAS E CERTIFICA√á√ïES
        if not clean:
            if curr_sec and any(k in curr_sec for k in ["IDIOMA", "IDIOMAS", "CERTIFICA√á√ÉO", "CERTIFICA√á√ïES"]):
                buffer.append(raw)
            continue

        upper = clean.upper()
        KEYWORDS = ["RESUMO", "SKILLS", "EXPERI√äNCIA", "HIST√ìRICO", "FORMA√á√ÉO", "IDIOMA", "CERTIFICA"]
        if any(k in upper for k in KEYWORDS) and len(clean) < 50:
            flush()
            curr_sec = upper
            continue

        if curr_sec:
            buffer.append(raw)

    flush()
    print(parsed)
    return parsed

# ==============================================================================
# 2. HTML GENERATOR
# ==============================================================================

def render_cv_html(parsed_data):
    """
    FONTE √öNICA DA VERDADE (HTML PURO).
    HTML final deve ficar VISUALMENTE ID√äNTICO √Ä TELA.
    """
    import re

    def normalize(txt: str) -> str:
        return re.sub(r'[^\w\s]', '', txt.lower()).strip()

    # --- CONTATOS ---
    contacts_html = ""
    if parsed_data.get("contacts"):
        raw = parsed_data["contacts"][0]
        raw = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', raw)
        raw = re.sub(r'\s*\|\s*', ' <span class="vant-contact-separator">‚Ä¢</span> ', raw)
        contacts_html = raw.replace('\n', ' ')

    sections_html = ""

    for sec in parsed_data.get("sections", []):
        content_html = ""

        # --- SKILLS ---
        if sec["type"] == "chips":
            content_html = (
                '<div class="vant-skills-container">' +
                ''.join(f'<span class="vant-skill-chip">{s}</span>' for s in sec.get("content", [])) +
                '</div>'
            )

        # --- PAR√ÅGRAFO ---
        elif sec["type"] == "paragraph":
            txt = re.sub(r'\*\*(.*?)\*\*', r'<span class="vant-bold">\1</span>', sec.get("content", ""))
            content_html = f'<div class="summary-text">{txt}</div>'

        # --- EXPERI√äNCIA ---
        elif sec["type"] == "jobs":
            jobs_html = ""

            for job in sec.get("content", []):
                bullets_html = ""

                cargo_norm = normalize(job.get("cargo", ""))

                raw_details = []

                for d in job.get("details", []):
                    parts = [p.strip() for p in d.split('|') if p.strip()]
                    raw_details.extend(parts)

                for raw in raw_details:
                    bullets_html += f"""
                    <div class="vant-cv-grid-row">
                        <div class="vant-cv-bullet-col">‚Ä¢</div>
                        <div class="vant-cv-text-col">{raw}</div>
                    </div>
                    """


                empresa = job.get("empresa", "")
                date = job.get("date", "")

                jobs_html += f"""
                <div class="vant-cv-job-container vant-avoid-break">
                    <div class="vant-job-title">
                        {job.get("cargo", "")}
                        <span class="vant-job-company"> | {empresa}</span>
                    </div>
                    <div class="vant-job-date">{date}</div>
                    {bullets_html}
                </div>
                """

            content_html = jobs_html

        # --- SE√á√ÉO ---
        sections_html += f"""
        <div class="vant-cv-section">{sec.get("title", "")}</div>
        {content_html}
        """

    return f"""
    <div class="cv-paper-sheet">
        <div class="vant-cv-name">{parsed_data.get("name", "")}</div>
        <div class="vant-cv-contact-line">{contacts_html}</div>
        {sections_html}
    </div>
    """
# ==============================================================================
# 3. FUN√á√ÉO PRINCIPAL (Interface com seu app.py)
# ==============================================================================

def gerar_pdf_candidato(data):
    """
    Gera PDF via WeasyPrint usando a MESMA estrutura da tela.
    """
    try:
        # 1. Obter texto bruto
        raw_text = data.get('cv_otimizado_completo', '')
        
        # 2. Obter o HTML do componente (O MESMO DA TELA)
        # USAR format_text_to_html DIRETAMENTE PARA GARANTIR FIDELIDADE VISUAL
        body_html = format_text_to_html(raw_text)
        
        # Envolver em div cv-paper-sheet para casar com o CSS
        body_html = f'<div class="cv-paper-sheet">{body_html}</div>'
        
        # 3. Criar o envelope HTML completo para o PDF
        full_html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <style>
                /* Injeta o CSS V13 aqui para garantir a formata√ß√£o */
                {CSS_V13}
            </style>
        </head>
        <body>
            {body_html}
        </body>
        </html>
        """
        
        # 4. Render PDF
        pdf_bytes = HTML(string=full_html).write_pdf(
            stylesheets=[CSS(string=CSS_PDF)]
        )
        return pdf_bytes

    except Exception as e:
        # Fallback mantido
        from fpdf import FPDF as FPDF_ERR
        err = FPDF_ERR(); err.add_page(); err.set_font("Arial", size=12)
        err.cell(0, 10, f"Erro WeasyPrint: {str(e)}", ln=True)
        return err.output(dest="S").encode("latin-1")
# ============================================================
# HELPER FUNCTIONS PARA AN√ÅLISE DE CV
# ============================================================

def _process_competitors(competitor_files):
    """Processa arquivos de concorrentes e retorna texto formatado."""
    competitors_text = ""
    if competitor_files:
        for i, comp_file in enumerate(competitor_files):
            c_text = extrair_texto_pdf(comp_file)
            if c_text:
                competitors_text += f"\n--- CONCORRENTE {i+1} ---\n{c_text[:15000]}\n"
        logger.info(f"‚öîÔ∏è Processando {len(competitor_files)} arquivos de concorr√™ncia.")
    return competitors_text

def _curate_books(area_detected):
    """Cura lista de livros baseada na √°rea detectada."""
    books_catalog = load_books_catalog()
    curated_books = []
    seen_titles = set()

    def add_book_safe(book_obj, origin):
        """Helper para adicionar apenas livros √∫nicos"""
        t = book_obj.get('titulo', '').strip().lower()
        if t and t not in seen_titles:
            seen_titles.add(t)
            b_enriched = book_obj.copy()
            b_enriched['categoria_origem'] = origin
            curated_books.append(b_enriched)

    # 1. SELE√á√ÉO T√âCNICA (Prioridade)
    specific_books = []
    if isinstance(books_catalog, dict):
        specific_books = books_catalog.get(area_detected, [])
    
    if not specific_books: # Backup
        specific_books = BACKUP_CATALOG.get(area_detected, [])
    
    for b in specific_books[:15]:
        add_book_safe(b, area_detected)

    # 2. SELE√á√ÉO SOFT SKILLS (Secund√°rio)
    soft_skills = []
    if isinstance(books_catalog, dict):
        soft_skills = books_catalog.get("global_soft_skills", [])
    
    if not soft_skills: # Backup
        soft_skills = BACKUP_CATALOG.get("global_soft_skills", [])
        
    for b in soft_skills[:5]:
        add_book_safe(b, "soft_skills")

    # 3. FALLBACK CATASTR√ìFICO
    if not curated_books:
        fallback_list = [
             {"titulo": "H√°bitos At√¥micos", "autor": "James Clear"},
             {"titulo": "Comece pelo Porqu√™", "autor": "Simon Sinek"}
        ]
        for b in fallback_list:
            add_book_safe(b, "fallback")

    return {"biblioteca_universal": curated_books}

# ============================================================
# ORQUESTRADOR BLINDADO (ATUALIZADO)
# ============================================================
def analyze_cv_logic(cv_text, job_description, competitor_files=None):
    
    # [DEV MODE - INICIO] -----------------------------------------
    DEV_MODE = os.getenv("VANT_DEV_MODE", "0") == "1"  # Toggle para testes r√°pidos sem gastar tokens

    if DEV_MODE:
        logger.info("üöß DEV MODE: Bypass de IA ativado.")
        return {
            "veredito": "APROVADO (MOCK)",
            "nota_ats": 88,
            "analise_por_pilares": {"impacto": 90, "keywords": 85, "ats": 95},
            "linkedin_headline": "Senior Software Engineer | Python | AWS",
            "resumo_otimizado": "Profissional focado em arquitetura escal√°vel...",
            "cv_otimizado_completo": """### Experi√™ncia Profissional...""",
            "gaps_fatais": [
                {"erro": "Falta de M√©tricas", "evidencia": "Texto vago", "correcao_sugerida": "Use **n√∫meros**."},
                {"erro": "Tecnologia Antiga", "evidencia": "Uso de SVN", "correcao_sugerida": "Migre para **Git**."}
            ],
            "analise_comparativa": {
                "vantagens_concorrentes": ["O Benchmark tem PMP.", "O S√™nior fala alem√£o."],
                "seus_diferenciais": ["Voc√™ domina **Python**.", "Voc√™ tem **Startup** no CV."],
                "plano_de_ataque": "Aposte na agilidade.",
                "probabilidade_aprovacao": 72
            },
            # [ATUALIZADO] Mock alinhado com a nova estrutura (sem roadmap)
            "biblioteca_tecnica": [{"titulo": "Clean Code", "autor": "Uncle Bob", "motivo": "Essencial."}],
            "perguntas_entrevista": [{"pergunta": "Fale sobre um erro.", "expectativa_recrutador": "Honestidade.", "dica_resposta": "Seja direto."}],
            "kit_hacker": {"boolean_string": "site:linkedin.com/in python"},
            "projeto_pratico": {
                "titulo": "API de Alta Performance",
                "descricao": "Criar uma API em FastAPI com cache Redis.",
                "como_apresentar": "Diga que reduziu lat√™ncia em 50ms."
            }
        }
    # [DEV MODE - FIM] --------------------------------------------
    
    
    # 1. Valida√ß√£o de Input (Fail Fast)
    if not cv_text or len(cv_text.strip()) < 50:
        logger.warning("Tentativa de an√°lise com CV vazio ou inv√°lido.")
        return {
            "veredito": "Erro de Leitura (Arquivo Vazio/Inv√°lido)",
            "nota_ats": 0,
            "gaps_fatais": [{"erro": "Arquivo Ileg√≠vel", "evidencia": "N√£o conseguimos extrair texto do PDF.", "correcao_sugerida": "Envie um PDF selecion√°vel (texto), n√£o escaneado."}]
        }

    # 2. PROCESSA CONCORRENTES
    competitors_text = _process_competitors(competitor_files)

    # 3. L√≥gica Original de Cat√°logo
    area_detected = detect_job_area(job_description)
    logger.info(f"üîé √Årea detectada: {area_detected.upper()}") 
    
    catalog_payload = _curate_books(area_detected)
    
    # Chama o orquestrador
    try:
        data = run_llm_orchestrator(
            cv_text=cv_text,
            job_description=job_description,
            books_catalog=catalog_payload,
            area=area_detected,
            # PASSANDO OS CONCORRENTES PARA O LLM
            competitors_text=competitors_text if competitors_text else None
        )
        return data
    except Exception as e:
        logger.error(f"Erro fatal no orquestrador: {e}")
        return {
            "veredito": "Erro T√©cnico",
            "nota_ats": 0,
            "gaps_fatais": [
                {
                    "erro": "Falha no Processamento",
                    "evidencia": str(e),
                    "correcao_sugerida": "Tente novamente mais tarde."
                }
            ]
        }
# [ATUALIZADO] ENGINE HEUR√çSTICO COM ENRIQUECIMENTO DE CARGO
# ============================================================
def analyze_preview_lite(cv_text, job_description):
    """
    Vers√£o FREE com IA REAL: Mostra nota + 2 gaps REAIS espec√≠ficos do CV.
    Objetivo: Provar valor antes de pedir pagamento.
    """
    import string
    import re
    import json
     
    # Prompt espec√≠fico para an√°lise gratuita (2 gaps reais)
    prompt_preview = f"""
Voc√™ √© um especialista em ATS e otimiza√ß√£o de curr√≠culos.

MISS√ÉO: Analisar este CV e identificar os 2 PROBLEMAS MAIS CR√çTICOS que est√£o impedindo aprova√ß√£o no ATS.

CV DO CANDIDATO:
{cv_text[:3000]}

VAGA ALVO:
{job_description[:1500]}

INSTRU√á√ïES:
1. Identifique os 2 problemas MAIS GRAVES e ESPEC√çFICOS deste CV
2. Use exemplos REAIS do texto do CV (n√£o invente)
3. Seja direto e objetivo
4. Foque em: falta de n√∫meros/resultados E palavras-chave ausentes

OUTPUT JSON (OBRIGAT√ìRIO):
{{
  "nota_ats": 0,
  "analise_por_pilares": {{
    "impacto": 0,
    "keywords": 0,
    "ats": 0
  }},
  "gap_1": {{
    "titulo": "Nome do problema",
    "explicacao": "Por que isso √© cr√≠tico",
    "exemplo_atual": "Trecho real do CV que mostra o problema",
    "exemplo_otimizado": "Como deveria ser (com n√∫meros e palavras-chave)"
  }},
  "gap_2": {{
    "titulo": "Nome do problema",
    "explicacao": "Por que isso √© cr√≠tico",
    "termos_faltando": ["termo1", "termo2", "termo3", "termo4", "termo5"]
  }}
}}

IMPORTANTE: Retorne APENAS o JSON, sem texto adicional.
"""
    
    try:
        # Chama a IA para an√°lise real
        response = run_llm_orchestrator(prompt_preview, max_tokens=1000)
        
        # Parse do JSON retornado pela IA
        # Remove markdown se houver
        response_clean = response.strip()
        if response_clean.startswith("```"):
            response_clean = response_clean.split("```")[1]
            if response_clean.startswith("json"):
                response_clean = response_clean[4:]
        
        data = json.loads(response_clean)
        
        # Extrai dados da IA
        nota_ats = int(data.get("nota_ats", 45))
        pilares = data.get("analise_por_pilares", {})
        gap_1 = data.get("gap_1", {})
        gap_2 = data.get("gap_2", {})
        
        # Detecta √°rea
        area_detected = detect_job_area(job_description)
        
        return {
            "veredito": "AN√ÅLISE INICIAL CONCLU√çDA",
            "nota_ats": nota_ats,
            "analise_por_pilares": {
                "impacto": int(pilares.get("impacto", nota_ats - 5)),
                "keywords": int(pilares.get("keywords", nota_ats)),
                "ats": int(pilares.get("ats", nota_ats + 5)),
                "setor_detectado": area_detected.upper()
            },
            "gap_1": gap_1,
            "gap_2": gap_2,
            "linkedin_headline": "üîí [CONTE√öDO PREMIUM BLOQUEADO]",
            "resumo_otimizado": "üîí [DISPON√çVEL APENAS NA VERS√ÉO PAGA]",
            "cv_otimizado_completo": "üîí",
            "kit_hacker": {"boolean_string": "üîí"},
            "biblioteca_tecnica": []
        }
        
    except Exception as e:
        logger.error(f"Erro na an√°lise preview com IA: {e}")
        
        # Fallback para heur√≠stica se a IA falhar
        def normalize(text):
            translator = str.maketrans('', '', string.punctuation)
            return set(text.lower().translate(translator).split())
         
        cv_tokens = normalize(cv_text)
        job_tokens = normalize(job_description)
         
        stopwords = {'a', 'e', 'o', 'de', 'do', 'da', 'em', 'para', 'com', 'que'}
        job_tokens = job_tokens - stopwords
         
        matches = cv_tokens.intersection(job_tokens)
        match_count = len(matches)
        total_relevant = len(job_tokens) if len(job_tokens) > 0 else 1
         
        raw_score = (match_count / total_relevant) * 100
        length_bonus = min(len(cv_text) / 500, 10)
         
        final_score = min(int(raw_score + length_bonus - 10), 65)
        final_score = max(final_score, 0)
         
        area_detected = detect_job_area(job_description)
        
        # Gaps gen√©ricos como fallback
        return {
            "veredito": "AN√ÅLISE INICIAL CONCLU√çDA",
            "nota_ats": final_score,
            "analise_por_pilares": {
                "impacto": max(final_score - 5, 0),
                "keywords": final_score,
                "ats": min(final_score + 5, 100),
                "setor_detectado": area_detected.upper()
            },
            "gap_1": {
                "titulo": "Falta de Resultados Quantific√°veis",
                "explicacao": "Seu CV usa descri√ß√µes gen√©ricas sem n√∫meros ou impacto mensur√°vel",
                "exemplo_atual": "Respons√°vel por gerenciar projetos e melhorar processos",
                "exemplo_otimizado": "Gerenciei 12 projetos com or√ßamento de R$ 2.5M, reduzindo custos em 28%"
            },
            "gap_2": {
                "titulo": "Palavras-Chave da Vaga Ausentes",
                "explicacao": "Termos cr√≠ticos da vaga n√£o aparecem no seu CV",
                "termos_faltando": ["Agile/Scrum", "KPIs", "Stakeholders", "Data-driven", "OKRs"]
            },
            "linkedin_headline": "üîí [CONTE√öDO PREMIUM BLOQUEADO]",
            "resumo_otimizado": "üîí [DISPON√çVEL APENAS NA VERS√ÉO PAGA]",
            "cv_otimizado_completo": "üîí",
            "kit_hacker": {"boolean_string": "üîí"},
            "biblioteca_tecnica": []
        }
 
# ============================================================
# GERADOR DE WORD V7 (DESIGN SYSTEM TRANSLATION)
# ============================================================
def gerar_word_candidato(data):
    """
    VERS√ÉO V10 EST√ÅVEL: Reorganizada e testada
    Corrige ordem de defini√ß√µes e remove depend√™ncias circulares
    """
    from docx import Document
    from docx.shared import Pt, RGBColor, Cm
    from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_TAB_ALIGNMENT
    from docx.oxml.ns import qn
    from docx.oxml import OxmlElement
    from io import BytesIO
    import re

    # ============================================================
    # CONSTANTES DE CORES E FONTES
    # ============================================================
    COLOR_TEXT   = RGBColor(51, 65, 85)
    COLOR_TITLE  = RGBColor(15, 23, 42)
    COLOR_ACCENT = RGBColor(16, 185, 129)
    COLOR_SUB    = RGBColor(100, 116, 139)
    COLOR_CHIP_TEXT = RGBColor(71, 85, 105)
    COLOR_BORDER_HEX = "E2E8F0"
    COLOR_CHIP_BG = "F1F5F9"
    FONT_MAIN = 'Segoe UI'

    # ============================================================
    # FUN√á√ïES HELPER DE VALIDA√á√ÉO (DEFINIDAS PRIMEIRO)
    # ============================================================
    
    def is_valid(text):
        """Valida se o texto n√£o √© lixo"""
        if not text: 
            return False
        t = str(text).strip().lower()
        garbage = ["n√£o informado", "nao informado", "n/a", "null", "none", ""]
        return t not in garbage and len(t) > 1
    
    def is_contact_line(text):
        """Detecta se √© linha de contato"""
        if not text:
            return False
        t = str(text).lower()
        keywords = ['@', 'linkedin', 'github', 'telefone', 'email', '.com']
        return any(kw in t for kw in keywords)

    # ============================================================
    # FUN√á√ïES HELPER XML (FORMATA√á√ÉO WORD)
    # ============================================================
    
    def set_border_bottom(paragraph):
        """Adiciona borda inferior ao par√°grafo"""
        try:
            p = paragraph._p
            pPr = p.get_or_add_pPr()
            pBdr = OxmlElement('w:pBdr')
            bottom = OxmlElement('w:bottom')
            bottom.set(qn('w:val'), 'single')
            bottom.set(qn('w:sz'), '4')
            bottom.set(qn('w:space'), '6')
            bottom.set(qn('w:color'), COLOR_BORDER_HEX)
            pBdr.append(bottom)
            pPr.append(pBdr)
        except:
            pass

    def apply_chip_background(run):
        """Aplica fundo colorido ao texto (chip)"""
        try:
            rPr = run._r.get_or_add_rPr()
            shd = OxmlElement('w:shd')
            shd.set(qn('w:val'), 'clear')
            shd.set(qn('w:color'), 'auto')
            shd.set(qn('w:fill'), COLOR_CHIP_BG)
            rPr.append(shd)
        except:
            pass

    def set_keep_with_next(paragraph):
        """Mant√©m par√°grafo grudado no pr√≥ximo"""
        try:
            p = paragraph._p
            pPr = p.get_or_add_pPr()
            keepNext = OxmlElement('w:keepNext')
            pPr.append(keepNext)
        except:
            pass

    def set_keep_together(paragraph):
        """Evita quebra dentro do par√°grafo"""
        try:
            p = paragraph._p
            pPr = p.get_or_add_pPr()
            keepLines = OxmlElement('w:keepLines')
            pPr.append(keepLines)
        except:
            pass

    # ============================================================
    # PARSER DE SKILLS (HEUR√çSTICA SIMPLIFICADA)
    # ============================================================
    
    def extract_skills_simple(text):
        """
        Extrai skills de texto corrido usando m√∫ltiplas estrat√©gias
        VERS√ÉO SIMPLIFICADA E ROBUSTA
        """
        if not text:
            return []
        
        try:
            # Limpa o texto
            text = re.sub(r'\s+', ' ', str(text)).strip()
            skills = []
            
            # ESTRAT√âGIA 1: Detecta palavras/frases capitalizadas
            # Exemplo: "An√°lise de Dados" "Automa√ß√£o de Processos"
            words = text.split()
            current_skill = []
            
            for word in words:
                # Se come√ßa com mai√∫scula ou √© conectora
                if word and (word[0].isupper() or word.lower() in ['de', 'e', 'da', 'do', 'em', 'com', 'a', 'o']):
                    current_skill.append(word)
                else:
                    # Finaliza skill atual
                    if current_skill:
                        skill_text = ' '.join(current_skill)
                        if len(skill_text) > 3 and len(skill_text) < 50:
                            skills.append(skill_text)
                    current_skill = []
            
            # Adiciona √∫ltima skill
            if current_skill:
                skill_text = ' '.join(current_skill)
                if len(skill_text) > 3 and len(skill_text) < 50:
                    skills.append(skill_text)
            
            # Remove duplicatas
            seen = set()
            unique = []
            for s in skills:
                s_lower = s.lower().strip()
                if s_lower not in seen and len(s) > 3:
                    seen.add(s_lower)
                    unique.append(s)
            
            return unique[:15]
        
        except Exception as e:
            # Se der erro, retorna lista vazia
            return []

    # ============================================================
    # INICIALIZA DOCUMENTO
    # ============================================================
    
    doc = Document()
    section = doc.sections[0]
    section.top_margin = Cm(1.5)
    section.bottom_margin = Cm(1.5)
    section.left_margin = Cm(1.8)
    section.right_margin = Cm(1.8)

    # ============================================================
    # PROCESSA TEXTO BRUTO
    # ============================================================
    
    raw_text = data.get('cv_otimizado_completo', '')
    if not raw_text:
        # Documento vazio se n√£o houver conte√∫do
        p = doc.add_paragraph("Curr√≠culo vazio")
        stream = BytesIO()
        doc.save(stream)
        stream.seek(0)
        return stream
    
    lines = raw_text.split('\n')
    
    # Estrutura para organizar o conte√∫do
    sections = []
    current_section = None
    buffer = []
    contact_lines = []
    name_found = False
    
    # ============================================================
    # PARSING LINHA POR LINHA
    # ============================================================
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # Remove markdown
        clean = line.replace('**', '').replace('*', '')
        
        # 1. DETECTA NOME (primeira linha ou linha com # )
        if not name_found:
            if clean.startswith('# '):
                sections.append({
                    'type': 'name',
                    'content': clean.replace('# ', '')
                })
                name_found = True
                continue
            elif len(clean) < 100 and not clean.startswith('|') and not is_contact_line(clean):
                # Primeira linha n√£o vazia = nome
                sections.append({
                    'type': 'name',
                    'content': clean
                })
                name_found = True
                continue
        
        # 2. DETECTA CONTATOS (antes da primeira se√ß√£o)
        if not current_section and is_contact_line(clean):
            contact_lines.append(clean)
            continue
        
        # 3. DETECTA SE√á√ÉO (| T√çTULO ou ### T√çTULO)
        if clean.startswith('|') or clean.startswith('###'):
            # Salva se√ß√£o anterior
            if current_section and buffer:
                sections.append({
                    'type': 'section',
                    'title': current_section,
                    'content': buffer.copy()
                })
            
            # Salva contatos se tiver
            if contact_lines:
                sections.append({
                    'type': 'contact',
                    'content': contact_lines.copy()
                })
                contact_lines = []
            
            # Nova se√ß√£o
            current_section = clean.replace('|', '').replace('###', '').strip()
            buffer = []
            continue
        
        # 4. ACUMULA CONTE√öDO
        if current_section:
            buffer.append(clean)
    
    # Salva √∫ltima se√ß√£o
    if current_section and buffer:
        sections.append({
            'type': 'section',
            'title': current_section,
            'content': buffer.copy()
        })

    # ============================================================
    # RENDERIZA DOCUMENTO
    # ============================================================
    
    for sec in sections:
        sec_type = sec.get('type', '')
        
        # ========== NOME ==========
        if sec_type == 'name':
            p = doc.add_paragraph()
            p.alignment = WD_ALIGN_PARAGRAPH.CENTER
            p.paragraph_format.space_after = Pt(4)
            
            r = p.add_run(sec['content'].upper())
            r.bold = True
            r.font.name = FONT_MAIN
            r.font.size = Pt(24)
            r.font.color.rgb = COLOR_TITLE
            set_keep_with_next(p)
        
        # ========== CONTATOS ==========
        elif sec_type == 'contact':
            for contact_line in sec.get('content', []):
                # Remove r√≥tulos
                clean = contact_line
                for label in ['Email:', 'Telefone:', 'LinkedIn:', 'GitHub:', 'Local:']:
                    clean = clean.replace(label, '')
                
                # Split por | ou ‚Ä¢
                parts = re.split(r'\s*[|‚Ä¢]\s*', clean)
                valid_parts = [p.strip() for p in parts if is_valid(p)]
                
                if valid_parts:
                    p = doc.add_paragraph()
                    p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                    p.paragraph_format.space_after = Pt(12)
                    
                    for i, part in enumerate(valid_parts):
                        r = p.add_run(part)
                        r.font.name = FONT_MAIN
                        r.font.size = Pt(9.5)
                        r.font.color.rgb = COLOR_SUB
                        
                        if i < len(valid_parts) - 1:
                            r_sep = p.add_run("  ‚Ä¢  ")
                            r_sep.font.color.rgb = COLOR_ACCENT
                            r_sep.bold = True
        
        # ========== SE√á√ïES ==========
        elif sec_type == 'section':
            title = sec.get('title', '').upper()
            content = sec.get('content', [])
            
            if not content:
                continue
            
            # T√≠tulo da se√ß√£o
            p = doc.add_paragraph()
            p.paragraph_format.space_before = Pt(12)
            p.paragraph_format.space_after = Pt(6)
            set_border_bottom(p)
            set_keep_with_next(p)
            
            r_marker = p.add_run("| ")
            r_marker.font.name = FONT_MAIN
            r_marker.font.size = Pt(14)
            r_marker.bold = True
            r_marker.font.color.rgb = COLOR_ACCENT
            
            r_title = p.add_run(title)
            r_title.bold = True
            r_title.font.name = FONT_MAIN
            r_title.font.size = Pt(11)
            r_title.font.color.rgb = COLOR_TITLE
            
            # ========== SKILLS (DETEC√á√ÉO ESPECIAL) ==========
            if any(kw in title for kw in ['SKILL', 'COMPET√äNCIA', 'T√âCNICA']):
                full_text = ' '.join(content)
                skills = extract_skills_simple(full_text)
                
                if skills:
                    p = doc.add_paragraph()
                    p.paragraph_format.space_after = Pt(8)
                    p.paragraph_format.line_spacing = 1.3
                    
                    for i, skill in enumerate(skills):
                        if i > 0:
                            p.add_run("  ")
                        
                        r_chip = p.add_run(f"  {skill}  ")
                        r_chip.font.name = FONT_MAIN
                        r_chip.font.size = Pt(9.5)
                        r_chip.font.color.rgb = COLOR_CHIP_TEXT
                        apply_chip_background(r_chip)
                else:
                    # Fallback: texto corrido
                    p = doc.add_paragraph(full_text)
                    p.paragraph_format.space_after = Pt(8)
                
                continue
            
            # ========== CONTE√öDO GERAL ==========
            for line in content:
                if not is_valid(line):
                    continue
                
                # CABE√áALHO DE EXPERI√äNCIA (tem |)
                if '|' in line and len(line) < 150:
                    # Remove bullet se tiver
                    line_clean = line
                    if line_clean.startswith('- '):
                        line_clean = line_clean[2:]
                    
                    parts = [p.strip() for p in line_clean.split('|')]
                    
                    # Valida (pelo menos 2 partes v√°lidas)
                    if len(parts) >= 2 and is_valid(parts[0]) and is_valid(parts[1]):
                        # Cargo + Empresa
                        p = doc.add_paragraph()
                        p.paragraph_format.space_before = Pt(10)
                        p.paragraph_format.space_after = Pt(1)
                        set_keep_together(p)
                        
                        r_cargo = p.add_run(parts[0])
                        r_cargo.bold = True
                        r_cargo.font.name = FONT_MAIN
                        r_cargo.font.size = Pt(11.5)
                        r_cargo.font.color.rgb = COLOR_TITLE
                        
                        r_sep = p.add_run(" | ")
                        r_sep.font.color.rgb = COLOR_SUB
                        
                        r_emp = p.add_run(parts[1])
                        r_emp.bold = True
                        r_emp.font.name = FONT_MAIN
                        r_emp.font.size = Pt(11)
                        r_emp.font.color.rgb = COLOR_ACCENT
                        
                        # Data (se tiver)
                        if len(parts) >= 3 and is_valid(parts[2]):
                            p_date = doc.add_paragraph()
                            p_date.paragraph_format.space_after = Pt(3)
                            
                            r_date = p_date.add_run(parts[2])
                            r_date.font.name = FONT_MAIN
                            r_date.font.size = Pt(9)
                            r_date.font.color.rgb = COLOR_SUB
                        
                        continue
                
                # BULLET POINT (descri√ß√£o de experi√™ncia)
                if line.startswith('- ') or line.startswith('‚Ä¢ '):
                    clean = line[2:].strip()
                    
                    p = doc.add_paragraph()
                    p.paragraph_format.space_after = Pt(2)
                    p.paragraph_format.line_spacing = 1.25
                    
                    # Hanging indent
                    p.paragraph_format.tab_stops.add_tab_stop(Cm(0.5), WD_TAB_ALIGNMENT.LEFT)
                    p.paragraph_format.left_indent = Cm(0.5)
                    p.paragraph_format.first_line_indent = Cm(-0.5)
                    set_keep_together(p)
                    
                    # Bullet
                    r_bullet = p.add_run("‚Ä¢")
                    r_bullet.font.name = FONT_MAIN
                    r_bullet.font.size = Pt(12)
                    r_bullet.font.color.rgb = COLOR_ACCENT
                    p.add_run("\t")
                    
                    # Texto (com negrito em t√≠tulos se tiver :)
                    if ':' in clean:
                        title_part, desc_part = clean.split(':', 1)
                        
                        r_title = p.add_run(title_part + ':')
                        r_title.bold = True
                        r_title.font.name = FONT_MAIN
                        r_title.font.size = Pt(10.5)
                        r_title.font.color.rgb = COLOR_TITLE
                        
                        r_desc = p.add_run(' ' + desc_part.strip())
                        r_desc.font.name = FONT_MAIN
                        r_desc.font.size = Pt(10.5)
                        r_desc.font.color.rgb = COLOR_TEXT
                    else:
                        r_txt = p.add_run(clean)
                        r_txt.font.name = FONT_MAIN
                        r_txt.font.size = Pt(10.5)
                        r_txt.font.color.rgb = COLOR_TEXT
                    
                    continue
                
                # TEXTO CORRIDO
                p = doc.add_paragraph()
                p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                p.paragraph_format.line_spacing = 1.25
                p.paragraph_format.space_after = Pt(6)
                
                r = p.add_run(line)
                r.font.name = FONT_MAIN
                r.font.size = Pt(10.5)
                r.font.color.rgb = COLOR_TEXT

    # ============================================================
    # SALVA E RETORNA
    # ============================================================
    
    stream = BytesIO()
    doc.save(stream)
    stream.seek(0)
    return stream